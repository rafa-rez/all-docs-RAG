# RAG para vasta maioria de docs da Ã¡rea de dados - IngestÃ£o e chat.

> ImplementaÃ§Ã£o simples, eficiente e **pronta para produÃ§Ã£o** de um pipeline **RAG (Retrievalâ€‘Augmented Generation)** com ingestÃ£o incremental, suporte a mÃºltiplos formatos e interface de debug via Streamlit.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![LangChain](https://img.shields.io/badge/LangChain-v0.2-green)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red)

---

##  VisÃ£o Geral

Este projeto abstrai a complexidade de criar um pipeline de dados para LLMs, oferecendo uma base sÃ³lida para **busca semÃ¢ntica, RAG e anÃ¡lise de documentos**.

Ele Ã© dividido em dois mÃ³dulos principais:

### ğŸ”¹ IngestÃ£o (`ingest.py`)
- Varredura automÃ¡tica de diretÃ³rios
- DetecÃ§Ã£o de alteraÃ§Ãµes via **hash MD5** (evita reprocessamento)
- ExtraÃ§Ã£o de metadados (ex: ano de referÃªncia)
- GeraÃ§Ã£o de embeddings locais
- PersistÃªncia em banco vetorial (ChromaDB)

### ğŸ”¹ Interface (`app.py`)
- Interface Streamlit simples e objetiva
- Testes de qualidade de retrieval
- VisualizaÃ§Ã£o das fontes recuperadas
- IntegraÃ§Ã£o com LLMs via Groq (Llama 3)

---

## Funcionalidades

-  **IngestÃ£o incremental** baseada em hash MD5  
-  **Suporte a mÃºltiplos formatos**: PDF, CSV e DOCX  
-  **CSV rowâ€‘based inteligente**  
  - Cada linha vira um documento semÃ¢ntico (keyâ€‘value)
  - Preserva significado e contexto dos dados tabulares  
-  **ExtraÃ§Ã£o automÃ¡tica de metadados** (ex: ano)
-  **Embeddings locais** com Sentenceâ€‘Transformers (sem custo de API)
-  **UI de debug** focada em transparÃªncia e rastreabilidade

---

##  Como Executar

###  PrÃ©â€‘requisitos
- Python **3.9+**
- Conta na **Groq** (para inferÃªncia com LLM)

---

###  InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/rafa-rez/all-docs-RAG.git
cd all-docs-RAG
```

2. Crie e ative um ambiente virtual (opcional, mas recomendado):
```bash
python -m venv venv
venv\Scripts\activate
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Configure as variÃ¡veis de ambiente  
Crie um arquivo `.env` na raiz do projeto:
```env
GROQ_API_KEY=sua_chave_aqui
```

---

##   Uso

###  Passo 1 â€” IngestÃ£o de Dados

Coloque seus arquivos (`PDF`, `CSV`, `DOCX`) na pasta `dados/` e execute:
```bash
python ingest.py
```

 Isso irÃ¡:
- Processar apenas arquivos novos ou modificados
- Gerar embeddings
- Criar o banco vetorial local em `./chroma_db_cache`

---

###  Passo 2 â€” Rodar a Interface

```bash
streamlit run app.py
```

A interface permitirÃ¡:
- Fazer perguntas aos documentos
- Avaliar a qualidade do retrieval
- Ver exatamente **quais trechos foram usados**

---

##  Estrutura do Projeto

```text
.
â”œâ”€â”€ dados/                  # Arquivos de entrada (PDF, CSV, DOCX)
â”œâ”€â”€ chroma_db_cache/        # Banco vetorial persistido (auto-gerado)
â”œâ”€â”€ ingest.py               # Pipeline de ingestÃ£o e embeddings
â”œâ”€â”€ app.py                  # Interface Streamlit
â”œâ”€â”€ controle_ingestao.json  # Cache de hashes MD5
â”œâ”€â”€ .env                    # VariÃ¡veis de ambiente
â””â”€â”€ requirements.txt        # DependÃªncias
```

---

##  Tecnologias

- **OrquestraÃ§Ã£o:** LangChain  
- **Vector Store:** ChromaDB  
- **Embeddings:** Sentenceâ€‘Transformers (HuggingFace)  
- **LLM:** Llama 3.1 (via Groq)  
- **Interface:** Streamlit  

---

O foco do projeto Ã© tornar a ingestÃ£o de dados nÃ£o estruturados mais fÃ¡cil para testes de datasets e lÃ³gicas.

---

##  Autor

Desenvolvido por **Rafael Rezende**  
